{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Deepfake Detection Pipeline - Google Colab\n",
    "\n",
    "Complete end-to-end deepfake detection system with training, evaluation, and inference.\n",
    "\n",
    "**Features:**\n",
    "- Train on FaceForensics++ dataset\n",
    "- Video-level predictions with frame aggregation\n",
    "- GPU acceleration (use Runtime > Change runtime type > GPU)\n",
    "- Pre-trained model inference\n",
    "- Visualization and metrics\n",
    "\n",
    "**Runtime Recommendation:** GPU (T4 or better) for faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision timm\n",
    "!pip install -q opencv-python-headless facenet-pytorch\n",
    "!pip install -q scikit-learn pandas matplotlib seaborn\n",
    "!pip install -q tqdm pyyaml\n",
    "\n",
    "print(\"âœ“ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected. Training will be slow. Enable GPU: Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository (or upload your files)\n",
    "!git clone https://github.com/YOUR_USERNAME/deepfake-detector.git\n",
    "%cd deepfake-detector\n",
    "\n",
    "# Or upload your local files:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload your project files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Choose one of the following options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option1"
   },
   "source": [
    "### Option A: Use Sample Dataset (Quick Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sample_data"
   },
   "outputs": [],
   "source": [
    "# Download a small sample dataset for quick testing\n",
    "!mkdir -p data/sample_videos\n",
    "!wget -O data/sample_videos/sample.zip https://YOUR_SAMPLE_DATASET_URL\n",
    "!unzip -q data/sample_videos/sample.zip -d data/sample_videos/\n",
    "\n",
    "print(\"âœ“ Sample data downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option2"
   },
   "source": [
    "### Option B: Upload Your Own Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_videos"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Upload videos\n",
    "print(\"Upload your videos (REAL and FAKE folders)...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Organize uploaded files\n",
    "!mkdir -p data/raw_videos/train/REAL\n",
    "!mkdir -p data/raw_videos/train/FAKE\n",
    "\n",
    "print(\"âœ“ Videos uploaded. Move them to data/raw_videos/train/REAL or FAKE folders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option3"
   },
   "source": [
    "### Option C: Mount Google Drive (Recommended for Large Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Link to your dataset in Google Drive\n",
    "# DATASET_PATH = '/content/drive/MyDrive/deepfake_data'\n",
    "# !ln -s $DATASET_PATH data/raw_videos\n",
    "\n",
    "print(\"âœ“ Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocess"
   },
   "source": [
    "## 3. Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract_frames"
   },
   "outputs": [],
   "source": [
    "# Extract frames from videos\n",
    "!python src/02_extract_frames.py \\\n",
    "    --input data/raw_videos \\\n",
    "    --output data/frames \\\n",
    "    --fps 3 \\\n",
    "    --max-frames 16\n",
    "\n",
    "print(\"âœ“ Frame extraction complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crop_faces"
   },
   "outputs": [],
   "source": [
    "# Detect and crop faces\n",
    "!python src/03_crop_faces.py \\\n",
    "    --input data/frames \\\n",
    "    --output data/frames \\\n",
    "    --size 224 \\\n",
    "    --device cuda\n",
    "\n",
    "print(\"âœ“ Face cropping complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train"
   },
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "!python src/04_train.py \\\n",
    "    --config config.yaml \\\n",
    "    --data data/frames \\\n",
    "    --output outputs \\\n",
    "    --epochs 10 \\\n",
    "    --batch-size 32 \\\n",
    "    --device cuda\n",
    "\n",
    "print(\"âœ“ Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view_history"
   },
   "outputs": [],
   "source": [
    "# View training history\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('outputs/metrics/training_history.json', 'r') as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "epochs = [h['epoch'] for h in history]\n",
    "train_f1 = [h['train']['f1'] for h in history]\n",
    "val_f1 = [h['val']['f1'] for h in history]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_f1, 'o-', label='Train F1', linewidth=2)\n",
    "plt.plot(epochs, val_f1, 's-', label='Val F1', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best validation F1: {max(val_f1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eval"
   },
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "!python src/05_eval.py \\\n",
    "    --checkpoint outputs/models/best.pt \\\n",
    "    --data data/frames \\\n",
    "    --output outputs/metrics \\\n",
    "    --split test\n",
    "\n",
    "print(\"âœ“ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view_results"
   },
   "outputs": [],
   "source": [
    "# View test results\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('outputs/metrics/test_report.csv', nrows=1)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {df['accuracy'].values[0]:.2%}\")\n",
    "print(f\"Precision: {df['precision'].values[0]:.2%}\")\n",
    "print(f\"Recall:    {df['recall'].values[0]:.2%}\")\n",
    "print(f\"F1 Score:  {df['f1'].values[0]:.2%}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_plots"
   },
   "outputs": [],
   "source": [
    "# Generate visualization plots\n",
    "!python src/07_report_plots.py \\\n",
    "    --metrics-dir outputs/metrics \\\n",
    "    --output-dir outputs/plots\n",
    "\n",
    "# Display plots\n",
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"\\nðŸ“Š Performance Metrics:\")\n",
    "display(Image('outputs/plots/prf_bar.png'))\n",
    "\n",
    "print(\"\\nðŸ“Š Confusion Matrix:\")\n",
    "display(Image('outputs/plots/confusion_matrix.png'))\n",
    "\n",
    "print(\"\\nðŸ“Š Training Curves:\")\n",
    "display(Image('outputs/plots/training_curves.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference"
   },
   "source": [
    "## 6. Inference on New Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_test_video"
   },
   "outputs": [],
   "source": [
    "# Upload a video to test\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Upload a video file to test...\")\n",
    "uploaded = files.upload()\n",
    "test_video = list(uploaded.keys())[0]\n",
    "print(f\"âœ“ Uploaded: {test_video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "predict"
   },
   "outputs": [],
   "source": [
    "# Run inference\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append('src/utils')\n",
    "from model import load_checkpoint\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, config = load_checkpoint('outputs/models/best.pt', device)\n",
    "model.eval()\n",
    "\n",
    "# Initialize face detector\n",
    "mtcnn = MTCNN(keep_all=False, device=device)\n",
    "\n",
    "# Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Extract frames\n",
    "cap = cv2.VideoCapture(test_video)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_interval = max(1, int(fps / 3))  # 3 fps\n",
    "frame_count = 0\n",
    "predictions = []\n",
    "\n",
    "print(f\"Processing {test_video}...\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    if frame_count % frame_interval == 0:\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect face\n",
    "        face = mtcnn(frame_rgb)\n",
    "        \n",
    "        if face is not None:\n",
    "            # Convert to PIL and apply transforms\n",
    "            face_pil = transforms.ToPILImage()(face)\n",
    "            face_tensor = transform(face_pil).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Predict\n",
    "            with torch.no_grad():\n",
    "                output = model(face_tensor)\n",
    "                prob = torch.sigmoid(output).item()\n",
    "                predictions.append(prob)\n",
    "    \n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Aggregate predictions\n",
    "if predictions:\n",
    "    avg_prob = np.mean(predictions)\n",
    "    result = \"FAKE\" if avg_prob > 0.5 else \"REAL\"\n",
    "    confidence = avg_prob if avg_prob > 0.5 else (1 - avg_prob)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"PREDICTION: {result}\")\n",
    "    print(f\"Confidence: {confidence:.2%}\")\n",
    "    print(f\"Fake Probability: {avg_prob:.2%}\")\n",
    "    print(f\"Frames Analyzed: {len(predictions)}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"âš ï¸ No faces detected in video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 7. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "# Download trained model\n",
    "from google.colab import files\n",
    "\n",
    "files.download('outputs/models/best.pt')\n",
    "print(\"âœ“ Model downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_results"
   },
   "outputs": [],
   "source": [
    "# Download all results as zip\n",
    "!zip -r outputs.zip outputs/\n",
    "files.download('outputs.zip')\n",
    "print(\"âœ“ All results downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_drive"
   },
   "source": [
    "## 8. Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_to_drive"
   },
   "outputs": [],
   "source": [
    "# Copy outputs to Google Drive\n",
    "!cp -r outputs /content/drive/MyDrive/deepfake_detector_outputs\n",
    "print(\"âœ“ Results saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tips"
   },
   "source": [
    "## Tips & Tricks\n",
    "\n",
    "### For Faster Training:\n",
    "1. Use GPU runtime (Runtime > Change runtime type > GPU)\n",
    "2. Use T4 or A100 GPU if available\n",
    "3. Increase batch size (if you have enough memory)\n",
    "4. Reduce max_frames if dataset is large\n",
    "\n",
    "### For Better Accuracy:\n",
    "1. Train on full FaceForensics++ dataset (9,000+ videos)\n",
    "2. Increase epochs to 15-20\n",
    "3. Use data augmentation (already enabled)\n",
    "4. Try different models: xception, efficientnet_b4\n",
    "\n",
    "### Common Issues:\n",
    "- **Out of Memory**: Reduce batch_size in training\n",
    "- **No faces detected**: Videos must contain clear faces\n",
    "- **Slow training**: Enable GPU runtime\n",
    "- **Session timeout**: Save checkpoints regularly to Drive\n",
    "\n",
    "### Next Steps:\n",
    "1. Deploy as REST API using Flask/FastAPI\n",
    "2. Build web interface with Gradio/Streamlit\n",
    "3. Fine-tune on specific deepfake types\n",
    "4. Test compression robustness (src/06_compress_test.py)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
